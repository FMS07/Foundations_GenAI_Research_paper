{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBUvWdJArgFN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust paths as necessary\n",
        "original_data = pd.read_csv('/content/preprocessed_twitter_data_small .csv')\n",
        "nemotron_data = pd.read_csv('/content/synthetic_data_nemotron.csv')\n",
        "mixtral_data = pd.read_csv('/content/synthetic_data_mixtral.csv')\n",
        "llama_data = pd.read_csv('/content/synthetic_data_llama.csv')\n"
      ],
      "metadata": {
        "id": "UC8Qz4Airq83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurations of different datasets\n",
        "data_configs = {\n",
        "    \"Original\": original_data,\n",
        "    \"Original + Nemotron\": pd.concat([original_data, nemotron_data], ignore_index=True),\n",
        "    \"Original + Mixtral\": pd.concat([original_data, mixtral_data], ignore_index=True),\n",
        "    \"Original + Llama\": pd.concat([original_data, llama_data], ignore_index=True),\n",
        "    \"All Combined\": pd.concat([original_data, nemotron_data, mixtral_data, llama_data], ignore_index=True)\n",
        "}\n"
      ],
      "metadata": {
        "id": "12WalbsSr72B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_synthetic_data(original, synthetic):\n",
        "    # Ensure the columns match and concatenate\n",
        "    synthetic_combined = pd.concat([original, synthetic[['synthetic_text', 'sentiment']]], ignore_index=True)\n",
        "    return synthetic_combined\n",
        "\n",
        "# Configurations of different dataset combinations\n",
        "data_configs = {\n",
        "    \"Original\": original_data,\n",
        "    \"Original + Nemotron\": concat_synthetic_data(original_data, nemotron_data),\n",
        "    \"Original + Mixtral\": concat_synthetic_data(original_data, mixtral_data),\n",
        "    \"Original + Llama\": concat_synthetic_data(original_data, llama_data),\n",
        "    \"All Combined\": pd.concat([\n",
        "        original_data,\n",
        "        nemotron_data[['synthetic_text', 'sentiment']],\n",
        "        mixtral_data[['synthetic_text', 'sentiment']],\n",
        "        llama_data[['synthetic_text', 'sentiment']]\n",
        "    ], ignore_index=True)\n",
        "}\n"
      ],
      "metadata": {
        "id": "FHtuCcJbs7q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(data):\n",
        "    # Using 'text' as the input feature and 'sentiment' as the target\n",
        "    X = data['text']\n",
        "    y = data['sentiment']\n",
        "\n",
        "    # Convert y to categorical for multi-class classification\n",
        "    y = pd.Categorical(y).codes  # Convert sentiment labels to integer codes\n",
        "    y = to_categorical(y, num_classes=4)  # One-hot encode for 4 classes\n",
        "\n",
        "    return train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "xz2ZKoIUsEcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(original_data['text'])\n",
        "\n",
        "def preprocess_text(X_train, X_test):\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "    max_length = max(len(seq) for seq in X_train_seq)  # Dynamic padding\n",
        "    X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
        "    X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
        "    return X_train_pad, X_test_pad\n"
      ],
      "metadata": {
        "id": "3n5kS9-Qtdge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='sigmoid'),\n",
        "        Dropout(0.5),\n",
        "        Dense(4, activation='softmax')  # Use 'softmax' for multi-class sentiment classification\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "T-roBUKdtiPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "for config_name, data in data_configs.items():\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = split_data(data)\n",
        "    # Preprocess text\n",
        "    X_train_pad, X_test_pad = preprocess_text(X_train, X_test)\n",
        "\n",
        "    # Create and train the model\n",
        "    model = create_model(X_train_pad.shape[1])\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    model.fit(X_train_pad, y_train, epochs=10, validation_split=0.2, batch_size=32, callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = np.argmax(model.predict(X_test_pad), axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)  # Convert one-hot to label indices\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')  # Weighted F1 for multi-class\n",
        "\n",
        "    # Save results\n",
        "    results[config_name] = {'Accuracy': acc, 'F1 Score': f1}\n",
        "    print(f\"{config_name} - Accuracy: {acc}, F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "f5VR0EBitldU",
        "outputId": "2e7b488d-663f-430c-d30e-cc9004d22da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 218ms/step - accuracy: 0.2125 - loss: 1.8652 - val_accuracy: 0.1875 - val_loss: 1.5255\n",
            "Epoch 2/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.2010 - loss: 1.9308 - val_accuracy: 0.2188 - val_loss: 1.5534\n",
            "Epoch 3/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2490 - loss: 1.8210 - val_accuracy: 0.2188 - val_loss: 1.5804\n",
            "Epoch 4/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2635 - loss: 1.8250 - val_accuracy: 0.1875 - val_loss: 1.5457\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step\n",
            "Original - Accuracy: 0.2, F1 Score: 0.20504532652701987\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'float' object has no attribute 'lower'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-082c1dac8392>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Preprocess text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mX_train_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Create and train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-aef322adff89>\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(X_train, X_test)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mX_train_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mX_test_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train_seq\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Dynamic padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_sequences\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_sequences_generator\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                     seq = text_to_word_sequence(\n\u001b[0m\u001b[1;32m    193\u001b[0m                         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                         \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m\"\"\"DEPRECATED.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtranslate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (if using Google Drive)\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical  # Import to_categorical\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the Datasets\n",
        "# Adjust paths as necessary\n",
        "original_data = pd.read_csv('/content/preprocessed_twitter_data_small .csv')\n",
        "nemotron_data = pd.read_csv('/content/synthetic_data_nemotron.csv')\n",
        "mixtral_data = pd.read_csv('/content/synthetic_data_mixtral.csv')\n",
        "llama_data = pd.read_csv('/content/synthetic_data_llama.csv')\n",
        "\n",
        "# Combine Synthetic Data with Original Data\n",
        "def concat_synthetic_data(original, synthetic):\n",
        "    # Ensure the columns match and concatenate\n",
        "    synthetic_combined = pd.concat([original, synthetic[['synthetic_text', 'sentiment']]], ignore_index=True)\n",
        "    return synthetic_combined\n",
        "\n",
        "# Configurations of different dataset combinations\n",
        "data_configs = {\n",
        "    \"Original\": original_data,\n",
        "    \"Original + Nemotron\": concat_synthetic_data(original_data, nemotron_data),\n",
        "    \"Original + Mixtral\": concat_synthetic_data(original_data, mixtral_data),\n",
        "    \"Original + Llama\": concat_synthetic_data(original_data, llama_data),\n",
        "    \"All Combined\": pd.concat([\n",
        "        original_data,\n",
        "        nemotron_data[['synthetic_text', 'sentiment']],\n",
        "        mixtral_data[['synthetic_text', 'sentiment']],\n",
        "        llama_data[['synthetic_text', 'sentiment']]\n",
        "    ], ignore_index=True)\n",
        "}\n",
        "\n",
        "# Define Train-Test Split Function\n",
        "def split_data(data):\n",
        "    # Using 'text' as the input feature and 'sentiment' as the target\n",
        "    X = data['text'].fillna('')  # Fill NaN values in 'text' with an empty string\n",
        "    y = data['sentiment']\n",
        "\n",
        "    # Convert y to categorical for multi-class classification\n",
        "    y = pd.Categorical(y).codes  # Convert sentiment labels to integer codes\n",
        "    y = to_categorical(y, num_classes=4)  # One-hot encode for 4 classes\n",
        "\n",
        "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Text Preprocessing (Tokenization)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(original_data['text'].fillna(''))  # Fit tokenizer on non-missing text data\n",
        "\n",
        "def preprocess_text(X_train, X_test):\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "    max_length = max(len(seq) for seq in X_train_seq)  # Dynamic padding\n",
        "    X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
        "    X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
        "    return X_train_pad, X_test_pad\n",
        "\n",
        "# Define Model Architecture\n",
        "def create_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_shape,)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(4, activation='softmax')  # Output layer for 4-class classification\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train and Evaluate on Each Dataset Configuration\n",
        "results = {}\n",
        "\n",
        "for config_name, data in data_configs.items():\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = split_data(data)\n",
        "    # Preprocess text\n",
        "    X_train_pad, X_test_pad = preprocess_text(X_train, X_test)\n",
        "\n",
        "    # Create and train the model\n",
        "    model = create_model(X_train_pad.shape[1])\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    model.fit(X_train_pad, y_train, epochs=15, validation_split=0.2, batch_size=32, callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = np.argmax(model.predict(X_test_pad), axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)  # Convert one-hot to label indices\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')  # Weighted F1 for multi-class\n",
        "\n",
        "    # Save results\n",
        "    results[config_name] = {'Accuracy': acc, 'F1 Score': f1}\n",
        "    print(f\"{config_name} - Accuracy: {acc}, F1 Score: {f1}\")\n",
        "\n",
        "# Display Results\n",
        "# Displaying all the results in a DataFrame for easier comparison\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg_EoG17uf-B",
        "outputId": "fbb09c05-c867-44c3-e560-5850423f5a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 209ms/step - accuracy: 0.2646 - loss: 179.6979 - val_accuracy: 0.3125 - val_loss: 85.0058\n",
            "Epoch 2/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1677 - loss: 173.9027 - val_accuracy: 0.3125 - val_loss: 81.2959\n",
            "Epoch 3/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.2948 - loss: 136.1296 - val_accuracy: 0.3125 - val_loss: 77.4042\n",
            "Epoch 4/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2073 - loss: 126.4144 - val_accuracy: 0.3125 - val_loss: 70.7206\n",
            "Epoch 5/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2917 - loss: 97.8422 - val_accuracy: 0.3125 - val_loss: 60.9189\n",
            "Epoch 6/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3656 - loss: 90.2444 - val_accuracy: 0.3125 - val_loss: 53.6192\n",
            "Epoch 7/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3375 - loss: 106.1123 - val_accuracy: 0.2812 - val_loss: 48.9799\n",
            "Epoch 8/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2562 - loss: 77.6063 - val_accuracy: 0.2812 - val_loss: 44.1018\n",
            "Epoch 9/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3698 - loss: 69.3187 - val_accuracy: 0.2812 - val_loss: 39.3195\n",
            "Epoch 10/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.2625 - loss: 91.0033 - val_accuracy: 0.2812 - val_loss: 35.9254\n",
            "Epoch 11/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3042 - loss: 58.1014 - val_accuracy: 0.2188 - val_loss: 33.6312\n",
            "Epoch 12/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2115 - loss: 53.5153 - val_accuracy: 0.2188 - val_loss: 32.1593\n",
            "Epoch 13/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3750 - loss: 50.7283 - val_accuracy: 0.2188 - val_loss: 30.8743\n",
            "Epoch 14/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3292 - loss: 51.4988 - val_accuracy: 0.2500 - val_loss: 30.2556\n",
            "Epoch 15/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2177 - loss: 58.5502 - val_accuracy: 0.2500 - val_loss: 30.1737\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
            "Original - Accuracy: 0.125, F1 Score: 0.13172043010752688\n",
            "Epoch 1/15\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 224ms/step - accuracy: 0.2725 - loss: 59.6771 - val_accuracy: 0.2480 - val_loss: 17.3651\n",
            "Epoch 2/15\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2299 - loss: 49.4730 - val_accuracy: 0.2240 - val_loss: 20.9306\n",
            "Epoch 3/15\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2487 - loss: 27.2620 - val_accuracy: 0.2400 - val_loss: 14.8823\n",
            "Epoch 4/15\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2715 - loss: 21.6735 - val_accuracy: 0.2640 - val_loss: 10.6106\n",
            "Epoch 5/15\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3190 - loss: 19.9867 - val_accuracy: 0.2800 - val_loss: 8.6772\n",
            "Epoch 6/15\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2854 - loss: 17.1773 - val_accuracy: 0.2720 - val_loss: 9.2840\n",
            "Epoch 7/15\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2403 - loss: 22.0376 - val_accuracy: 0.3040 - val_loss: 8.5801\n",
            "Epoch 8/15\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2618 - loss: 15.3929 - val_accuracy: 0.3040 - val_loss: 8.7284\n",
            "Epoch 9/15\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2530 - loss: 16.0696 - val_accuracy: 0.2880 - val_loss: 7.8558\n",
            "Epoch 10/15\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2211 - loss: 16.0829 - val_accuracy: 0.2960 - val_loss: 5.8801\n",
            "Epoch 11/15\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2715 - loss: 10.1043 - val_accuracy: 0.2880 - val_loss: 5.6729\n",
            "Epoch 12/15\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2452 - loss: 14.8115 - val_accuracy: 0.2960 - val_loss: 5.2182\n",
            "Epoch 13/15\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2485 - loss: 12.8331 - val_accuracy: 0.2880 - val_loss: 5.4476\n",
            "Epoch 14/15\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2397 - loss: 10.7869 - val_accuracy: 0.2800 - val_loss: 5.3824\n",
            "Epoch 15/15\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2695 - loss: 11.7396 - val_accuracy: 0.2720 - val_loss: 4.0890\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "Original + Nemotron - Accuracy: 0.27388535031847133, F1 Score: 0.18280399208515677\n",
            "Epoch 1/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 665ms/step - accuracy: 0.2504 - loss: 189.1677 - val_accuracy: 0.3333 - val_loss: 73.5101\n",
            "Epoch 2/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2599 - loss: 175.8112 - val_accuracy: 0.3590 - val_loss: 55.8176\n",
            "Epoch 3/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3127 - loss: 108.0472 - val_accuracy: 0.2821 - val_loss: 48.9209\n",
            "Epoch 4/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3557 - loss: 91.2941 - val_accuracy: 0.2821 - val_loss: 44.6953\n",
            "Epoch 5/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2949 - loss: 96.1761 - val_accuracy: 0.2821 - val_loss: 38.8917\n",
            "Epoch 6/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2849 - loss: 88.8156 - val_accuracy: 0.2821 - val_loss: 35.2193\n",
            "Epoch 7/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2702 - loss: 72.0519 - val_accuracy: 0.2821 - val_loss: 32.8635\n",
            "Epoch 8/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3280 - loss: 84.2176 - val_accuracy: 0.3077 - val_loss: 30.5830\n",
            "Epoch 9/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.2497 - loss: 65.1857 - val_accuracy: 0.3077 - val_loss: 28.2902\n",
            "Epoch 10/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3575 - loss: 51.0872 - val_accuracy: 0.2564 - val_loss: 26.0653\n",
            "Epoch 11/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3257 - loss: 51.0929 - val_accuracy: 0.2564 - val_loss: 24.1731\n",
            "Epoch 12/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3184 - loss: 49.4448 - val_accuracy: 0.2564 - val_loss: 23.5067\n",
            "Epoch 13/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2775 - loss: 57.1202 - val_accuracy: 0.2564 - val_loss: 23.6937\n",
            "Epoch 14/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3979 - loss: 45.9765 - val_accuracy: 0.2821 - val_loss: 23.0212\n",
            "Epoch 15/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4245 - loss: 35.3505 - val_accuracy: 0.2564 - val_loss: 21.5521\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step\n",
            "Original + Mixtral - Accuracy: 0.20833333333333334, F1 Score: 0.1800976800976801\n",
            "Epoch 1/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 650ms/step - accuracy: 0.2716 - loss: 229.4212 - val_accuracy: 0.2308 - val_loss: 65.2627\n",
            "Epoch 2/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.2286 - loss: 154.4779 - val_accuracy: 0.2821 - val_loss: 38.7734\n",
            "Epoch 3/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3294 - loss: 126.8987 - val_accuracy: 0.3077 - val_loss: 43.0673\n",
            "Epoch 4/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3185 - loss: 111.9779 - val_accuracy: 0.3333 - val_loss: 39.4813\n",
            "Epoch 5/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3068 - loss: 124.6089 - val_accuracy: 0.3333 - val_loss: 35.9356\n",
            "Epoch 6/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.2043 - loss: 124.7482 - val_accuracy: 0.2821 - val_loss: 30.2443\n",
            "Epoch 7/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3307 - loss: 82.9360 - val_accuracy: 0.2564 - val_loss: 27.4231\n",
            "Epoch 8/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2781 - loss: 83.5972 - val_accuracy: 0.3077 - val_loss: 25.4617\n",
            "Epoch 9/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2868 - loss: 77.3114 - val_accuracy: 0.2821 - val_loss: 24.3183\n",
            "Epoch 10/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3016 - loss: 65.8501 - val_accuracy: 0.3333 - val_loss: 23.5179\n",
            "Epoch 11/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2378 - loss: 63.4919 - val_accuracy: 0.3333 - val_loss: 22.1052\n",
            "Epoch 12/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3164 - loss: 52.0374 - val_accuracy: 0.3590 - val_loss: 21.0696\n",
            "Epoch 13/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3398 - loss: 56.6198 - val_accuracy: 0.3846 - val_loss: 20.3969\n",
            "Epoch 14/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2686 - loss: 67.4350 - val_accuracy: 0.3333 - val_loss: 19.5964\n",
            "Epoch 15/15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3833 - loss: 43.3147 - val_accuracy: 0.3333 - val_loss: 18.8427\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
            "Original + Llama - Accuracy: 0.22916666666666666, F1 Score: 0.22855263157894737\n",
            "Epoch 1/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.2280 - loss: 57.0432 - val_accuracy: 0.2319 - val_loss: 19.6263\n",
            "Epoch 2/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2621 - loss: 31.5711 - val_accuracy: 0.2246 - val_loss: 21.8155\n",
            "Epoch 3/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2672 - loss: 21.1205 - val_accuracy: 0.2101 - val_loss: 17.3527\n",
            "Epoch 4/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2683 - loss: 24.8996 - val_accuracy: 0.2174 - val_loss: 15.5457\n",
            "Epoch 5/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2406 - loss: 21.2379 - val_accuracy: 0.2246 - val_loss: 12.2931\n",
            "Epoch 6/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2746 - loss: 15.4749 - val_accuracy: 0.2246 - val_loss: 9.3845\n",
            "Epoch 7/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2581 - loss: 11.6149 - val_accuracy: 0.2536 - val_loss: 7.7434\n",
            "Epoch 8/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2652 - loss: 16.7470 - val_accuracy: 0.2319 - val_loss: 7.3486\n",
            "Epoch 9/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2569 - loss: 11.6379 - val_accuracy: 0.2391 - val_loss: 5.8527\n",
            "Epoch 10/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2592 - loss: 13.0971 - val_accuracy: 0.2391 - val_loss: 6.1035\n",
            "Epoch 11/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3193 - loss: 8.7311 - val_accuracy: 0.2464 - val_loss: 5.3420\n",
            "Epoch 12/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2376 - loss: 11.6206 - val_accuracy: 0.2391 - val_loss: 5.6322\n",
            "Epoch 13/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3008 - loss: 8.8503 - val_accuracy: 0.2464 - val_loss: 5.5636\n",
            "Epoch 14/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2568 - loss: 8.0197 - val_accuracy: 0.2391 - val_loss: 5.4891\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "All Combined - Accuracy: 0.28901734104046245, F1 Score: 0.1822178942573552\n",
            "                     Accuracy  F1 Score\n",
            "Original             0.125000  0.131720\n",
            "Original + Nemotron  0.273885  0.182804\n",
            "Original + Mixtral   0.208333  0.180098\n",
            "Original + Llama     0.229167  0.228553\n",
            "All Combined         0.289017  0.182218\n"
          ]
        }
      ]
    }
  ]
}