{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jdvg78k_cBy0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10599bcd-dbb4-4078-ef54-8abfcdcbeabe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
            "Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/106.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.11.0\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.4-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from textstat) (75.1.0)\n",
            "Downloading textstat-0.7.4-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.17.0 textstat-0.7.4\n"
          ]
        }
      ],
      "source": [
        "!pip install groq\n",
        "!pip install vaderSentiment\n",
        "!pip install textstat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from groq import Groq\n",
        "\n",
        "# Set up your Groq API key\n",
        "os.environ[\"GROQ_API_KEY\"] = \"\"  # or directly pass as Groq(api_key=\"your_groq_api_key\")\n",
        "\n",
        "# Initialize the Groq client\n",
        "client = Groq()\n",
        "\n",
        "# Function to generate synthetic text using multiple examples as context\n",
        "def generate_synthetic_text(input_examples):\n",
        "    # Format the examples into a prompt context\n",
        "    context = \"\\n\".join([f\"- {text}\" for text in input_examples])\n",
        "    prompt = f\"Based on the following examples of text with a similar sentiment:\\n{context}\\nGenerate a new example with the same sentiment. Please enclose the example in double quotes.\"\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"llama-3.1-70b-versatile\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ],\n",
        "        temperature=1,\n",
        "        max_tokens=1024,\n",
        "        top_p=1,\n",
        "        stream=True,\n",
        "        stop=None,\n",
        "    )\n",
        "\n",
        "    generated_text = \"\"\n",
        "    for chunk in completion:\n",
        "        generated_text += chunk.choices[0].delta.content or \"\"\n",
        "\n",
        "    # Extract only the text enclosed in double quotes\n",
        "    quoted_text = re.search(r'\"(.*?)\"', generated_text)\n",
        "    return quoted_text.group(1) if quoted_text else None\n",
        "\n",
        "# Load your dataset\n",
        "data_path = '/content/preprocessed_twitter_data_small (1).csv'\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "# Define the column names\n",
        "sentiment_column = 'sentiment'  # Replace with the actual sentiment column name\n",
        "text_column = 'text'  # Replace with the actual text column name\n",
        "\n",
        "# Create synthetic data for each sentiment class\n",
        "synthetic_data = []\n",
        "sentiment_classes = data[sentiment_column].unique()  # Extract unique sentiment classes\n",
        "\n",
        "for sentiment in sentiment_classes:\n",
        "    # Select all rows of this sentiment to use as context\n",
        "    sentiment_data = data[data[sentiment_column] == sentiment][text_column].tolist()\n",
        "\n",
        "    # Generate 10 synthetic rows for this sentiment class\n",
        "    for i in range(10):\n",
        "        synthetic_text = generate_synthetic_text(sentiment_data[:10])  # Use the first 10 examples as context\n",
        "        if synthetic_text:  # Only add if extraction was successful\n",
        "            synthetic_data.append({\n",
        "                \"original_text\": sentiment_data[i % 10],  # Store each example from the first 10 as original text\n",
        "                \"synthetic_text\": synthetic_text,\n",
        "                \"sentiment\": sentiment\n",
        "            })\n",
        "\n",
        "# Convert synthetic data to DataFrame\n",
        "synthetic_df = pd.DataFrame(synthetic_data)\n",
        "\n",
        "# Save synthetic data to CSV\n",
        "synthetic_df.to_csv('/content/synthetic_data_llama.csv', index=False)\n",
        "print(\"Synthetic data saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G04bRkAVdBzU",
        "outputId": "0169222d-9c45-44e9-f114-a52f01a8356a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic data saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perplexity, Lexical Diversity and Embedding Similarity"
      ],
      "metadata": {
        "id": "CHFfjyYndTP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the synthetic data (make sure this file path is correct)\n",
        "synthetic_data_path = '/content/synthetic_data_llama.csv'\n",
        "synthetic_data = pd.read_csv(synthetic_data_path)\n",
        "\n",
        "# Initialize GPT-2 model for perplexity calculation\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Initialize Sentence Transformer model for embedding similarity\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Function to calculate perplexity\n",
        "def calculate_perplexity(text):\n",
        "    inputs = gpt2_tokenizer(text, return_tensors=\"pt\")\n",
        "    outputs = gpt2_model(**inputs, labels=inputs[\"input_ids\"])\n",
        "    loss = outputs.loss\n",
        "    perplexity = torch.exp(loss)\n",
        "    return perplexity.item()\n",
        "\n",
        "# Function to calculate lexical diversity, handling empty text\n",
        "def lexical_diversity(text):\n",
        "    words = text.split()\n",
        "    return len(set(words)) / len(words) if len(words) > 0 else 0\n",
        "\n",
        "# Calculate metrics for each row in the synthetic dataset\n",
        "perplexities_original = []\n",
        "perplexities_synthetic = []\n",
        "lexical_diversities_original = []\n",
        "lexical_diversities_synthetic = []\n",
        "embedding_similarities = []\n",
        "\n",
        "for index, row in synthetic_data.iterrows():\n",
        "    original_text = row['original_text']\n",
        "    synthetic_text = row['synthetic_text']\n",
        "\n",
        "    # Perplexity\n",
        "    perplexities_original.append(calculate_perplexity(original_text))\n",
        "    perplexities_synthetic.append(calculate_perplexity(synthetic_text))\n",
        "\n",
        "    # Lexical Diversity\n",
        "    lexical_diversities_original.append(lexical_diversity(original_text))\n",
        "    lexical_diversities_synthetic.append(lexical_diversity(synthetic_text))\n",
        "\n",
        "    # Embedding Similarity\n",
        "    original_embedding = embedding_model.encode([original_text])[0]\n",
        "    synthetic_embedding = embedding_model.encode([synthetic_text])[0]\n",
        "    similarity = cosine_similarity([original_embedding], [synthetic_embedding])[0][0]\n",
        "    embedding_similarities.append(similarity)\n",
        "\n",
        "# Add calculated metrics to the dataframe\n",
        "synthetic_data['perplexity_original'] = perplexities_original\n",
        "synthetic_data['perplexity_synthetic'] = perplexities_synthetic\n",
        "synthetic_data['lexical_diversity_original'] = lexical_diversities_original\n",
        "synthetic_data['lexical_diversity_synthetic'] = lexical_diversities_synthetic\n",
        "synthetic_data['embedding_similarity'] = embedding_similarities\n",
        "\n",
        "# Calculate average metrics grouped by sentiment\n",
        "grouped_metrics = synthetic_data.groupby('sentiment').agg({\n",
        "    'perplexity_original': 'mean',\n",
        "    'perplexity_synthetic': 'mean',\n",
        "    'lexical_diversity_original': 'mean',\n",
        "    'lexical_diversity_synthetic': 'mean',\n",
        "    'embedding_similarity': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Display the grouped metrics\n",
        "print(\"Comparison of Original and Synthetic Text Metrics by Sentiment Type:\")\n",
        "print(grouped_metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfJINmI7dQ86",
        "outputId": "e79593db-9c3f-4a17-f41b-8d3d415525f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison of Original and Synthetic Text Metrics by Sentiment Type:\n",
            "    sentiment  perplexity_original  perplexity_synthetic  \\\n",
            "0  Irrelevant          3805.482076             90.143864   \n",
            "1    Negative           324.332158             58.703008   \n",
            "2     Neutral           501.634111            117.940827   \n",
            "3    Positive           247.962753             33.712287   \n",
            "\n",
            "   lexical_diversity_original  lexical_diversity_synthetic  \\\n",
            "0                    0.949769                     0.954778   \n",
            "1                    0.953587                     0.975301   \n",
            "2                    0.964069                     0.981607   \n",
            "3                    0.983687                     0.988235   \n",
            "\n",
            "   embedding_similarity  \n",
            "0              0.186310  \n",
            "1              0.196073  \n",
            "2              0.099976  \n",
            "3              0.184331  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VADAR Sentiment Analysis"
      ],
      "metadata": {
        "id": "XD5SFVSSdduA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install vaderSentiment\n",
        "import pandas as pd\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to calculate sentiment intensity\n",
        "def sentiment_strength(text):\n",
        "    return analyzer.polarity_scores(text)['compound']\n",
        "\n",
        "# Load the synthetic data (make sure this file path is correct)\n",
        "synthetic_data_path = '/content/synthetic_data_llama.csv'\n",
        "synthetic_data = pd.read_csv(synthetic_data_path)\n",
        "\n",
        "real_data=pd.read_csv('/content/preprocessed_twitter_data_small (1).csv')\n",
        "\n",
        "# Apply sentiment strength analysis on synthetic and original data\n",
        "synthetic_data['sentiment_strength'] = synthetic_data['synthetic_text'].apply(sentiment_strength)\n",
        "real_data['sentiment_strength'] = real_data['text'].apply(sentiment_strength)\n",
        "\n",
        "# Compare the average sentiment strength by sentiment type\n",
        "strength_comparison = synthetic_data.groupby('sentiment')['sentiment_strength'].mean()\n",
        "print(\"Average Sentiment Strength Comparison by Sentiment Type:\")\n",
        "print(strength_comparison)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIPA5JPmdhGi",
        "outputId": "636bb60f-5773-4992-8f5d-1472f24b61e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Sentiment Strength Comparison by Sentiment Type:\n",
            "sentiment\n",
            "Irrelevant    0.18370\n",
            "Negative     -0.37350\n",
            "Neutral      -0.19392\n",
            "Positive      0.67157\n",
            "Name: sentiment_strength, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N Gram Analysis"
      ],
      "metadata": {
        "id": "4JpL2P0mdk1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Calculate bigram overlap\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "\n",
        "# Transform the real data to extract bigrams\n",
        "original_bigrams = vectorizer.fit_transform(real_data['text']).toarray()\n",
        "synthetic_bigrams = vectorizer.transform(synthetic_data['synthetic_text']).toarray()\n",
        "\n",
        "# Sum along the rows to get the total count of each bigram across the dataset\n",
        "original_bigrams_sum = original_bigrams.sum(axis=0)\n",
        "synthetic_bigrams_sum = synthetic_bigrams.sum(axis=0)\n",
        "\n",
        "# Calculate overlap as a percentage by taking the minimum count of each bigram and dividing by the maximum\n",
        "overlap = np.sum(np.minimum(original_bigrams_sum, synthetic_bigrams_sum)) / np.sum(np.maximum(original_bigrams_sum, synthetic_bigrams_sum)) * 100\n",
        "\n",
        "print(\"Bigram Overlap Percentage between Original and Synthetic Texts:\", overlap)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTPRxY9Vdn60",
        "outputId": "a15db179-aa68-4a14-975a-a4f248519eb7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Overlap Percentage between Original and Synthetic Texts: 2.3126463700234194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Consistency"
      ],
      "metadata": {
        "id": "krpYf8VrdqNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import pandas as pd\n",
        "\n",
        "# Load combined data with original and synthetic texts\n",
        "file_path = '/content/synthetic_data_llama.csv'  # Update with actual path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Initialize LDA and TF-IDF Vectorizer\n",
        "n_topics = 5  # Adjust the number of topics as necessary\n",
        "lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "\n",
        "# Function to calculate topic distribution for a specific sentiment type and dataset\n",
        "def calculate_topic_distribution(data_subset, text_column):\n",
        "    tfidf_matrix = vectorizer.fit_transform(data_subset[text_column])\n",
        "    topic_distribution = lda.fit_transform(tfidf_matrix)\n",
        "    return topic_distribution.mean(axis=0)\n",
        "\n",
        "# Dictionary to store results\n",
        "comparison_results = {}\n",
        "\n",
        "# List of unique sentiments\n",
        "sentiments = data['sentiment'].unique()\n",
        "\n",
        "# Analyze each sentiment type\n",
        "for sentiment in sentiments:\n",
        "    # Filter data for the current sentiment\n",
        "    original_data_sentiment = data[data['sentiment'] == sentiment]['original_text']\n",
        "    synthetic_data_sentiment = data[data['sentiment'] == sentiment]['synthetic_text']\n",
        "\n",
        "    # Calculate topic distribution for original and synthetic data for this sentiment\n",
        "    original_topic_dist = calculate_topic_distribution(data[data['sentiment'] == sentiment], 'original_text')\n",
        "    synthetic_topic_dist = calculate_topic_distribution(data[data['sentiment'] == sentiment], 'synthetic_text')\n",
        "\n",
        "    # Store results\n",
        "    comparison_results[sentiment] = {\n",
        "        'Original Topic Distribution': original_topic_dist,\n",
        "        'Synthetic Topic Distribution': synthetic_topic_dist\n",
        "    }\n",
        "\n",
        "# Display results\n",
        "for sentiment, dist in comparison_results.items():\n",
        "    print(f\"\\nSentiment: {sentiment}\")\n",
        "    print(\"Original Topic Distribution:\", dist['Original Topic Distribution'])\n",
        "    print(\"Synthetic Topic Distribution:\", dist['Synthetic Topic Distribution'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtUUegX-dtBD",
        "outputId": "ed90ab65-8606-4f98-da27-c40b0311d218"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentiment: Irrelevant\n",
            "Original Topic Distribution: [0.3565141  0.1786207  0.20403984 0.21300903 0.04781633]\n",
            "Synthetic Topic Distribution: [0.12239071 0.2002812  0.20501089 0.27614345 0.19617375]\n",
            "\n",
            "Sentiment: Negative\n",
            "Original Topic Distribution: [0.12169552 0.33359285 0.12797208 0.20553899 0.21120056]\n",
            "Synthetic Topic Distribution: [0.28335702 0.19844521 0.20051218 0.19824464 0.11944095]\n",
            "\n",
            "Sentiment: Neutral\n",
            "Original Topic Distribution: [0.18886666 0.12060077 0.12064597 0.2027136  0.36717301]\n",
            "Synthetic Topic Distribution: [0.2024838  0.12098591 0.35576861 0.19889396 0.12186772]\n",
            "\n",
            "Sentiment: Positive\n",
            "Original Topic Distribution: [0.13622398 0.22522698 0.29807291 0.12899818 0.21147795]\n",
            "Synthetic Topic Distribution: [0.12582722 0.20387084 0.20408298 0.3390149  0.12720406]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diversity of Generated Text"
      ],
      "metadata": {
        "id": "A3vFCLEpdx9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the path to your file containing both original and synthetic data\n",
        "file_path = '/content/synthetic_data_llama.csv'  # Update with actual file path\n",
        "\n",
        "# Load the data\n",
        "combined_data = pd.read_csv(file_path)\n",
        "\n",
        "# Calculate uniqueness within the synthetic data itself\n",
        "unique_synthetic_texts = combined_data['synthetic_text'].nunique()\n",
        "total_synthetic_texts = len(combined_data['synthetic_text'])\n",
        "self_uniqueness_ratio = unique_synthetic_texts / total_synthetic_texts * 100\n",
        "\n",
        "# Calculate overlap between original and synthetic data\n",
        "overlapping_texts = combined_data[combined_data['synthetic_text'].isin(combined_data['original_text'])].shape[0]\n",
        "original_to_synthetic_overlap_ratio = (overlapping_texts / total_synthetic_texts) * 100\n",
        "\n",
        "# Print the results\n",
        "print(\"Uniqueness Ratio of Synthetic Data (compared to itself):\", self_uniqueness_ratio, \"%\")\n",
        "print(\"Overlap Ratio between Original and Synthetic Data:\", original_to_synthetic_overlap_ratio, \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBhyfK4td0DP",
        "outputId": "1fdc4bd8-eda3-4252-fee3-150e85591537"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uniqueness Ratio of Synthetic Data (compared to itself): 100.0 %\n",
            "Overlap Ratio between Original and Synthetic Data: 0.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Readability Scores"
      ],
      "metadata": {
        "id": "MLekhJTGd5XS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install textstat library\n",
        "\n",
        "import pandas as pd\n",
        "from textstat import flesch_reading_ease, flesch_kincaid_grade\n",
        "\n",
        "# Load the combined dataset containing both original and synthetic text columns\n",
        "file_path = '/content/synthetic_data_llama.csv'  # Update with the actual file path\n",
        "combined_data = pd.read_csv(file_path)\n",
        "\n",
        "# Calculate readability for synthetic and original texts separately\n",
        "combined_data['flesch_reading_ease_original'] = combined_data['original_text'].apply(flesch_reading_ease)\n",
        "combined_data['flesch_reading_ease_synthetic'] = combined_data['synthetic_text'].apply(flesch_reading_ease)\n",
        "\n",
        "# Compare average readability by sentiment type for both original and synthetic texts\n",
        "readability_comparison = combined_data.groupby('sentiment')[\n",
        "    ['flesch_reading_ease_original', 'flesch_reading_ease_synthetic']\n",
        "].mean()\n",
        "\n",
        "# Print the readability comparison by sentiment\n",
        "print(\"Average Readability Comparison by Sentiment Type:\")\n",
        "print(readability_comparison)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V549Gocd9bI",
        "outputId": "7d0c9787-258d-47e0-f9a0-de5cb8bfdf6b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Readability Comparison by Sentiment Type:\n",
            "            flesch_reading_ease_original  flesch_reading_ease_synthetic\n",
            "sentiment                                                              \n",
            "Irrelevant                        70.560                         78.012\n",
            "Negative                          65.778                         65.627\n",
            "Neutral                           67.858                         71.549\n",
            "Positive                          95.657                         69.383\n"
          ]
        }
      ]
    }
  ]
}